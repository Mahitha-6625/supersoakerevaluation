# -*- coding: utf-8 -*-
"""WISE ML Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dRwOasH9AV7Zi_9a7cLwSC2MYYpoBEwj

**Importing Libraries and Dataset**
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import roc_auc_score,classification_report,roc_curve,auc
from sklearn.linear_model import LogisticRegression
import seaborn as sns
from sklearn.datasets import make_classification

from google.colab import drive
drive.mount('/content/drive')

data = pd.read_csv('/content/drive/MyDrive/WISE ML/train.csv')
test = pd.read_csv('/content/drive/MyDrive/WISE ML/test.csv')

"""**Studying dataset**"""

data.head()

data.shape

data['failure'].value_counts()

data.isna().sum().sum()

data.info()

print(f'''This Dataset has features:

Categorical Features : {len(data.describe(include=object).columns)}
Numerical Features : {len(data.describe().columns)}

This Dataset has {len(data.columns)} Features
''')

"""**Data Preprocessing and Visualization**

**#Encoding categorical Data**
"""

le = LabelEncoder()
for feature in data.columns:
    if data[feature].dtype == object:
        data[feature] = le.fit_transform(data[feature])
data.head(10)

le = LabelEncoder()
for feature in test.columns:
    if test[feature].dtype == object:
        test[feature] = le.fit_transform(test[feature])
test.head(10)

"""**# To visualize NaN values in each feature**"""

nan_df = pd.DataFrame(data.isna().sum(), columns=['missing'])
nan_df['percent'] = (nan_df['missing'] / len(data)) * 100
plt.figure(figsize=(15, 10))
plt.bar(x=nan_df.index, height = nan_df['missing'])
plt.xticks(rotation='vertical')
plt.show()
nan_df

"""**# Replacing NaN values with the mean of that particular feature(column)**"""

for feature in data.columns:
    if data[feature].dtype != object:
        data[feature].fillna(value=round(data[feature].mean(), 2), inplace=True)

data.isna().sum().sum()

for feature in test.columns:
    if test[feature].dtype != object:
        test[feature].fillna(value=round(test[feature].mean(), 2), inplace=True)

test.isna().sum().sum()

"""**#Visualizing the coorelation between the features using Heat Map**"""

plt.figure(figsize=(15,8))
sns.heatmap(data.corr(),fmt='.1f',annot=True)
plt.show()

"""**# To visualize the distribution**"""

figure = plt.figure(figsize=(30, 25))
for i in range(len(data.columns)):
    plt.subplot(5, 6, i+1)
    plt.hist(data[data.columns[i]], bins=100)
    plt.title(data.columns[i])

plt.show()

#data.head()

data = data.drop(columns=['product_code'])

test = test.drop(columns=['product_code'])

features = data.drop(columns=['failure'])
label = data[['failure']]
from imblearn.over_sampling import SMOTE

feature,label = SMOTE().fit_resample(features,label)

from sklearn.model_selection import train_test_split
trainx,testx,trainy,testy = train_test_split(feature,label,train_size=.8)

"""**Building Models**

**#Logistic Regression**
"""

lgr=LogisticRegression(random_state=666)
lgr.fit(trainx,trainy)
lr_probs = lgr.predict_proba(testx)
lr_probs = lr_probs[:, 1]
y_score = lgr.predict(testx)
lgraccuracy = lgr.score(testx, testy)
print(lgraccuracy)

# getting the auc roc curve
auc = roc_auc_score(testy, y_score)
print('Classification Report:')
print(classification_report(testy,y_score))
false_positive_rate, true_positive_rate, thresholds = roc_curve(testy, y_score)
lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)
print('ROC_AUC_SCORE is',roc_auc_score(testy, y_score))

#fpr, tpr, _ = roc_curve(y_test, predictions[:,1])

plt.plot(false_positive_rate, true_positive_rate)
plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

"""**#Random Forest Classifier**"""

clf = RandomForestClassifier(n_estimators=500,criterion="gini",random_state=42)
clf.fit(trainx, trainy)
y_scores = clf.predict(testx)
lr_probs = clf.predict_proba(testx)
lr_probs = lr_probs[:, 1]
clfaccuracy = clf.score(testx, testy)
print(clfaccuracy)

# getting the auc roc curve
auc = roc_auc_score(testy, y_scores)
print('Classification Report:')
print(classification_report(testy,y_scores))
false_positive_rate, true_positive_rate, thresholds = roc_curve(testy, y_scores)
lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)
print('ROC_AUC_SCORE is',roc_auc_score(testy, y_scores))

#fpr, tpr, _ = roc_curve(y_test, predictions[:,1])

plt.plot(false_positive_rate, true_positive_rate)
plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

"""**#XGBoost**"""

import xgboost as xgb
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from xgboost import XGBClassifier

model = XGBClassifier()
model.fit(trainx, trainy)

y_pred = model.predict(testx)
lr_probs = model.predict_proba(testx)
lr_probs

lr_probs = lr_probs[:, 1]
lr_probs

xgbaccuracy = model.score(testx, testy)
print(xgbaccuracy)

# getting the auc roc curve
auc = roc_auc_score(testy, y_pred)
print('Classification Report:')
print(classification_report(testy,y_pred))
false_positive_rate, true_positive_rate, thresholds = roc_curve(testy, y_pred)
lr_fpr, lr_tpr, _ = roc_curve(testy, lr_probs)
print('ROC_AUC_SCORE is',roc_auc_score(testy, y_pred))

#fpr, tpr, _ = roc_curve(y_test, predictions[:,1])

plt.plot(false_positive_rate, true_positive_rate)
plt.plot(lr_fpr, lr_tpr, marker='.', label='Logistic')
plt.xlabel('FPR')
plt.ylabel('TPR')
plt.title('ROC curve')
plt.show()

"""**#Comparing the accuracy and ROC score of three models**"""

resultsLGR_test = pd.DataFrame({'Accuracy':lgraccuracy,'ROC Score': roc_auc_score(testy, y_score)},index=['Logistic Regression'])
resultsXGB_test = pd.DataFrame({'Accuracy':xgbaccuracy,'ROC Score': roc_auc_score(testy, y_pred)},index=['XGBoost Classifier'])
resultsCLF_test = pd.DataFrame({'Accuracy':clfaccuracy,'ROC Score': roc_auc_score(testy, y_scores)},index=['RandomForest Classifier'])
results_test = pd.concat([resultsLGR_test,resultsXGB_test,resultsCLF_test])
results_test

"""**Studying test dataset**"""

test.head()

"""**Predicting the failure using best classifier,i.e, Random forest classifier**"""

pred = clf.predict(test)

test['failure'] = pred

test.head(10)

test['failure'].value_counts()

import pandas as pd

# Create the submission DataFrame
submission = pd.DataFrame({'Index': test.id, 'failure': test.failure})

# Export the DataFrame to a CSV file
submission.to_csv('submission.csv', index=False)

df1 = pd.read_csv('/content/submission.csv')
df1.head()

import pandas as pd

# Create the submission DataFrame
submission = pd.DataFrame({'id': list(range(len(df1['Index']))), 'Index': test.id, 'failure': test.failure})

# Export the DataFrame to a CSV file
submission.to_csv('submission.csv', index=False)

df1 = pd.read_csv('/content/submission.csv')
df1.head()

df1['failure'].value_counts()

"""**Gradio**"""

!pip install gradio

import gradio as gr
import pickle

pickle.dump(clf,open('csv.sav','wb'))
model = pickle.load(open('csv.sav','rb'))

def predict_res(id):
    g = (test.loc[id])
    res = clf.predict((g[:][:-1]).to_numpy().reshape(1, -1))
    if res == 0:
        return "Not a failure product"
    else :
        return "Failure product"

title = "Super Soaker Evaluation"
description = """
<center>
The bot is trained to detection whether a product is Failure or not
<img src="https://images.hobbydb.com/processed_uploads/subject_photo/subject_photo/image/40834/1530044368-5270-8452/NerfSuperSoakerLogo_large.jpg" width="300" height="200">
Give the ID of the product
</center>
"""

outputs = gr.outputs.Textbox()

gr.Interface(predict_res, inputs="number", outputs=outputs,title = title,description = description).launch()